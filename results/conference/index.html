<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Ontology Alignment Evaluation Initiative::Conference track|evaluation</title>


<script type="text/javascript" src="./conference_files/jquery-latest.js"></script>
<link rel="stylesheet" type="text/css" href="./conference_files/jquery.css">
<script type="text/javascript" language="javascript" src="./conference_files/jquery.js"></script>
<link rel="stylesheet" type="text/css" href="./conference_files/style.css">
<style type="text/css" class="init">
</style>
<script type="text/javascript" class="init">
    $(document).ready(function() {
	$('#tab-rar2-M3').dataTable({
	  "paging":   false,
	  "info":     false
        });
    $('#tab').dataTable({
	  "paging":   false,
	  "info":     false
        });    
    } );
</script>
<script src="./conference_files/prompt.js"></script></head><body>

<div class="header">
<a style="color: grey; line-height: 5mm;" href="oaei.ontologymatching.org/2024/">Ontology
  Alignment Evaluation Initiative - OAEI-2024
  Campaign</a><a href="oaei.ontologymatching.org/"><img src="./conference_files/oaeismall.jpg" alt="OAEI" style="border-style: none; float: right; margin-left: 5pt;"></a>
  <img src="./conference_files/seals-logo.jpg" alt="OAEI" style="clear:right;float:right; margin-left: 5pt; border-style:none;" width="200px">
</div>

<h1>Results of Evaluation for the Conference track within OAEI 2024</h1>

<p>
Web page content
</p>
<ul>
<li><a href="https://oaei.ontologymatching.org/2024/results/conference/#participants">Participants</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#data">Data</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#modalities">Evaluation Modalities</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#crisp-ra">Evaluation Based on Crisp Reference Alignments</a>
<ul>
<li><a href="https://oaei.ontologymatching.org/2024/results/conference/#setting">Evaluation Setting and Tables Description</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#ref-comparison">Comparison of OAEI 2023 to 2024</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#discussion">Discussion for Evaluation Based on Crisp Reference Alignments</a>
</li></ul>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#uncertain-ra">Evaluation Based on the Uncertain Version of the Reference Alignment</a>
<ul>
<li><a href="https://oaei.ontologymatching.org/2024/results/conference/#uncsetting">Evaluation Setting</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#uncresults">Results</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#uncdiscussion">Discussion for Evaluation Based on the Uncertain Reference Alignments</a>
</li></ul>
 </li><li><a href="#logical">Evaluation Based on Logical Reasoning</a> 
<!-- </li><li><a href="#dbpedia">Experiment and Evaluation of DBpedia to OntoFarm Ontologies Matching</a> -->
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#organizers">Organizers</a>
</li><li><a href="https://oaei.ontologymatching.org/2024/results/conference/#references">References</a>
</li></ul>

<h2><a name="participants">Participants</a></h2><a name="participants">
<p>
This year, there were 7 participants (ALIN, LogMap, LogMapLt, Matcha, MDMapper, OntoMatch, and TOMATO) that managed to generate meaningful output. These are the matchers that were submitted with the ability to run the Conference Track. 
MDMapper and OntoMatch are two new matchers participating in this year.
<!-- We also provide comparison with tools that participated in previous years of OAEI in terms of the highest average F1-measure. -->
</p>

<h3>Important Notes for Reproducibility</h3>
  <li>Resulted alignments of ALIN, LogMap, LogMapLt, Matcha, MDMapper, and TOMATO come from MELT.</li>
  <li>Resulted alignments of OntoMatch were generated by following the instructions provided by the authors.</li>

</a><h2><a name="participants"></a><a name="data">Data</a></h2>

<h3>Participants Alignments</h3>

<p>You can <a href="./conference_files/conference-track-2024.zip">download</a> a subset of all alignments for which there is a reference alignment. We provide the alignments as generated by the MELT platform and in case of OntoMatch, we modified the generated alignments to match the MELT outputs. Alignments are stored as follows: SYSTEM-ontology1-ontology2.rdf.</p>

<h2><a name="modalities">Evaluation Modalities</a></h2>
<p>
Tools have been evaluated based on</p>
<ul>
<li><a href="./#crisp-ra">crisp reference alignments</a> where the confidence values for all matches are 1.0.
</li><li><a href="./#uncertain-ra">the uncertain version of the reference alignment</a> where confidence values reflect the degree of agreement of a group of twenty people on the validity of the match [1], 
<!-- </li><li><a href="#logical">logical reasoning</a> using violations of consistency and conservativity principles (presented at OWLED 2014 and at ISWC 2014 [2,3]).  -->
</li></ul>

<h3><a name="crisp-ra">Evaluation Based on Crisp Reference Alignments</a></h3>
<p>
We have three variants of crisp reference alignments (the confidence values for all matches are 1.0). They contain 21 alignments (test cases), which corresponds to the complete alignment space between 7 ontologies from the OntoFarm data set. This is a subset of all ontologies within this track (16) [4], see <a href="owl.vse.cz/ontofarm/"> OntoFarm data set web page</a>.</p>
<p>
<b>Here, we only publish the results based on the main (blind) reference alignment (rar2-M3). This will also be used within the synthesis paper.</b>
</p>
<h4><a name="rar2-M3">rar2-M3</a></h4>
<!-- <img src="eval_data/rar2-M3x.png" alt="rar2-M3"> -->

<div id="tab-rar2-M3_wrapper" class="dataTables_wrapper no-footer"><div id="tab-rar2-M3_filter" class="dataTables_filter"><label>Search:<input type="search" class="" placeholder="" aria-controls="tab-rar2-M3"></label></div><table id="tab-rar2-M3" class="display dataTable no-footer" cellspacing="0" width="80%" role="grid" style="width: 80%;">
<thead><tr role="row"><th class="sorting_asc" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-sort="ascending" aria-label="Matcher: activate to sort column ascending" style="width: 188px;">Matcher</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="Threshold: activate to sort column ascending" style="width: 172px;">Threshold</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="Precision: activate to sort column ascending" style="width: 162px;">Precision</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="F.5-measure: activate to sort column ascending" style="width: 200px;">F.5-measure</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="F1-measure: activate to sort column ascending" style="width: 195px;">F1-measure</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="F2-measure: activate to sort column ascending" style="width: 195px;">F2-measure</th><th class="sorting" tabindex="0" aria-controls="tab-rar2-M3" rowspan="1" colspan="1" aria-label="Recall: activate to sort column ascending" style="width: 117px;">Recall</th></tr></thead><tbody>

<tr role="row" class="odd"><td class="sorting_1">TOMATO</td><td>0.0</td><td> 0.57</td><td> 0.53</td><td> 0.48</td><td> 0.44</td><td> 0.42</td></tr>
<tr role="row" class="even"><td class="sorting_1">StringEquiv</td><td>0.0</td><td> 0.76</td><td> 0.65</td><td> 0.53</td><td> 0.45</td><td> 0.41</td></tr>
<tr role="row" class="odd"><td class="sorting_1">MDMapper</td><td>0.0</td><td> 0.66</td><td> 0.63</td><td> 0.59</td><td> 0.55</td><td> 0.53</td></tr>
<tr role="row" class="even"><td class="sorting_1">OntoMatch</td><td>0.0</td><td> 0.82</td><td> 0.69</td><td> 0.56</td><td> 0.48</td><td> 0.43</td></tr>
<tr role="row" class="odd"><td class="sorting_1">LogMapLt</td><td>0.0</td><td> 0.68</td><td> 0.62</td><td> 0.56</td><td> 0.5</td><td> 0.47</td></tr>
<tr role="row" class="even"><td class="sorting_1">MATCHA</td><td>0.0</td><td> 0.66</td><td> 0.65</td><td> 0.64</td><td> 0.64</td><td> 0.63</td></tr>
<tr role="row" class="odd"><td class="sorting_1">ALIN</td><td>0.0</td><td> 0.82</td><td> 0.7</td><td> 0.57</td><td> 0.48</td><td> 0.44</td></tr>
<tr role="row" class="even"><td class="sorting_1">LogMap</td><td>0.0</td><td> 0.76</td><td> 0.71</td><td> 0.64</td><td> 0.59</td><td> 0.56</td></tr>
<tr role="row" class="odd"><td class="sorting_1">edna</td><td>0.0</td><td> 0.74</td><td> 0.66</td><td> 0.56</td><td> 0.49</td><td> 0.45</td></tr>
</tbody></table>
</tbody></table></div>

<p>For the crisp reference alignment evaluation, you can see <a href="./eval.html" target="_new">more details</a> - we provide three evaluation variants for each reference alignment.</p>
<!--
<ul>
<li><b>ra1</b> is the original reference alignment which can be <a href="oaei.ontologymatching.org/2019/conference/data/reference-alignment.zip">downloaded</a>
 - please let us know how you use this reference-alignment (outside the OAEI context) and data set (ondrej.zamazal at vse dot cz).
</li><li><b>ra2</b> is an entailed reference alignment (ra2) generated as a transitive closure computed on the original reference alignment (ra1). In order to obtain coherent reference alignment set, conflicting correspondences have been inspected and resolved by evaluators. As a result the degree of correctness and completeness of ra2 is probaly slightly better than for ra1. However, the differences are relatively restricted.
</li><li><b>rar2</b> is a violation free version of reference alignment (ra2). First violating correspondences have been detected using approach from [2, 3] and then carefully resolved by an evaluator.
</li></ul>

<p>For each reference alignment we provide three evaluation variants</p>
<ul>
<li><b>M1</b> only contains classes,
</li><li><b>M2</b> only contains properties,
</li><li><b>M3</b> contains classes and properties, i.e. M3 = M1 + M2.
</li></ul>

<p>rar2 M3 is used as the main reference alignment for this year. It will also be used within the synthesis paper.</p>

<table border="1">
<tbody><tr><td></td><td>ra1</td><td>ra2</td><td>rar2</td></tr>
<tr><td>M1</td><td><a href="#ra1-M1">ra1-M1</a></td><td><a href="#ra2-M1">ra2-M1</a></td><td><a href="#rar2-M1">rar2-M1</a></td></tr>
<tr><td>M2</td><td><a href="#ra1-M2">ra1-M2</a></td><td><a href="#ra2-M2">ra2-M2</a></td><td><a href="#rar2-M2">rar2-M2</a></td></tr>
<tr><td>M3</td><td><a href="#ra1-M3">ra1-M3</a></td><td><a href="#ra2-M3">ra2-M3</a></td><td><a href="#rar2-M3">rar2-M3</a></td></tr>
</tbody></table>
-->
<h4><a name="setting">Evaluation Setting and Tables Description</a></h4>
<p>
Regarding evaluation based on reference alignment, we first filtered out (from alignments generated using MELT platform) all instance-to-any_entity and owl:Thing-to-any_entity correspondences prior to computing Precision/Recall/F1-measure/F2-measure/F0.5-measure because they are not contained in the reference alignment. In order to compute average Precision and Recall over all those alignments, we used absolute scores (i.e. we computed precision and 
recall using absolute scores of TP, FP, and FN across all 21 test cases). This corresponds to micro average precision and recall. Therefore, the resulting numbers can slightly differ with those computed by the MELT platform as macro average precision and recall. Then, we computed F1-measure in a standard way. Finally, we found the highest average F1-measure with thresholding (if possible).
</p>

<p>
In order to provide some context for understanding matchers performance, we included two simple string-based matchers as baselines. StringEquiv (before it was called Baseline1) is a string matcher based on string equality applied on local names of entities which were lowercased before (this baseline was also used within anatomy track 2012), and edna 
(string editing distance matcher) was adopted from benchmark track (wrt. performance it is very similar to the previously used baseline2).
</p>

<h4><a name="discussion">Discussion for Evaluation Based on Crisp Reference Alignments</a></h4>
<p>
With regard to the two baselines, we can group tools according to matcher's position (above best edna baseline, above StringEquiv baseline, below StringEquiv baseline), sorted by F1-measure. Regarding tools position, all tools keep the same position in ra1-M3, ra2-M3 and rar2-M3. There are six matchers above (or equal to) edna baseline (ALIN, LogMap, LogMapLT, Matcha, MDMapper, and OntoMatch), and one matcher below StringEquiv baseline (TOMATO). Since rar2 is not only consistency violation free (as ra2) but also conservativity violation free, we consider the rar2 as main reference alignment for this year. It will also be used within the synthesis paper.
</p>

<p>
Based on the evaluation variants M1 and M2, three matchers (ALIN, MDMapper, and OntoMatch) do not match properties at all.
</p>
<p>For the crisp reference alignment evaluation, you can see <a href="./eval.html" target="_new">more details</a> - we provide three evaluation variants for each reference alignment.</p>

<h4><a name="ref-comparison">Comparison of OAEI 2023 and 2024</a></h4>
<p>Table below summarizes performance results of tools that participated in the last 2 years of OAEI Conference track with regard to reference alignment rar2.</p>
<img src="./conference_files/comparison1-rar2-M3.png" alt="Perfomance results summary OAEI 2024 and 2023">
<p>
Based on this evaluation, we can see that three of the matching tools (ALIN, LogMap, LogMapLt) did not change the results. TOMATO slightly decreased its F1-measure and recall. Matcha slightly increased all three, precision, F1-measure, and recall.
</p>
<img src="./conference_files/comparison2-rar2-M3.png" alt="Perfomance results summary OAEI 2024 and 2023">

<!--<img src="eval_data/comparison2-rar2-M3.png" alt="Difference between 2023 and 2020 results">-->
<!--
<p>
In the tables below, there are results of all 10 tools with regard to all combinations of evaluation variants with crisp reference alignments. There are precision, recall, F1-measure, F2-measure and F0.5-measure computed for the threshold that provides the highest average F1-measure computed for each matcher. F1-measure is the harmonic mean of precision and recall. F2-measure (for beta=2) weights recall higher than precision and F0.5-measure (for beta=0.5) weights precision higher than recall.
</p>
-->

<!-- <p>Tools are ordered according to their highest average F1-measure. According to matcher's position with regard to two baselines it can be in one of three basic groups:</p> -->
<!--
<h4><a name="ra1-M1">ra1-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra1-M1.png" alt="ra1-M1">

<h4><a name="ra1-M2">ra1-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra1-M2.png" alt="ra1-M2">

<h4><a name="ra1-M3">ra1-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra1-M3.png" alt="ra1-M3">

<h4><a name="ra2-M1">ra2-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra2-M1.png" alt="ra2-M1">

<h4><a name="ra2-M2">ra2-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra2-M2.png" alt="ra2-M2">

<h4><a name="ra2-M3">ra2-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/ra2-M3.png" alt="ra2-M3">

<h4><a name="rar2-M1">rar2-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/rar2-M1.png" alt="rar2-M1">

<h4><a name="rar2-M2">rar2-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/rar2-M2.png" alt="rar2-M2">

<h4><a name="rar2-M3">rar2-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="eval_data/rar2-M3.png" alt="rar2-M3">

<h4><a name="ref-comparison">Comparison of OAEI 2020 and 2019</a></h4>
<p>Table below summarizes performance results of tools that participated in the last 2 years of OAEI Conference track with regard to reference alignment rar2.</p>
<img src="eval_data/comparison1-rar2-M3.png" alt="Perfomance results summary OAEI 2020 and 2019">
<p>
Based on this evaluation, we can see that four of the matching tools did not change the results, one very slightly improved (Wiktionary) and one very slightly improved its precision but decreased its recall and F1-measure (Lily).
</p>

<img src="eval_data/comparison2-rar2-M3.png" alt="Difference between 2020 and 2019 results">
-->

<h4><a name="visualization">Results visualization on precision/recall triangular graph based on the rar2-M3 reference alignment</a></h4>
<p>
All tools are visualized in terms of their performance regarding an average F1-measure in the figure below. Tools are represented as squares or triangles. Baselines are represented as circles. Horizontal line depicts level of precision/recall while values of average F1-measure are depicted by areas bordered by corresponding lines F1-measure=0.[5|6|7].
</p>
<img src="evaluation_files/triangular_rar2_M3_2024.png" alt="precision/recall triangular graph for conference and F1-measure based on rar2-M3" width="500" height="600">

<!--
<p>
Based on the evaluation variants M1 and M2, two matchers (ALIN and Lily) do not match properties at all.
Naturally, this has a negative effect on overall tools performance within the M3 evaluation variant.
</p>
-->
<!--
<h4><a name="false-positives">False Positives</h4>
<p>
Additionally, we analysed the False Positives (FPs) - alignments discovered by the tools which were evaluated as incorrect. The list of the False Positives can be viewed <a href="FPs_all.htm">here</a> in the following structure: entity / concept 1 (C1), entity / concept 2 (C2), reason why the alignment was discovered (Why Found) - only for a subset introduced later in this section, number of tools that returned the mapping (Tools) and individual tools - if the alignment was discovered by the tool, it is marked with an "x".
</p>
<p>
We looked at the reasons why an alignment was discovered from a general point of view, and defined 4 reasons why they could have been chosen.
</p>
<p>
Why was an alignment discovered:
<ul>
	<li>same name - concepts have identical names</li>
	<li>contains same word - names of the concepts include the same word or the word in another form</li>
	<li>synonym - names of the concepts include synonyms</li>
	<li>similar string - names of the concepts include similar strings (e.g., trip, tip)</li>
</ul>
</p>
<p>
	The following table provides some basic statictics for each system concerning the False Positives. It shows the number of FPs for each system (all FPs, FPs concerning classes only and FPs concerning properties), the number of unique FPs discovered by an system which were not discovered by any other systems (again for all FPs, FPs concerning classes only and FPs concerning properties), and lastly, the numbers of FPs divided by their suggested discovery reason. Hovewer, due to the larger number of FPs generated by DESKMatcher, only a subset of the FPs was used to obtain the DESKMatcher's numbers in the last category. This subset excludes those FPs that were discovered uniquely by DESKMatcher and can be viewed <a href="FPs_reduced.htm">here</a>.
</p>
<img src="eval_data/FP_statistics.png" alt="Statistics of False Positives">
<p>
	Additionally, a subset of FPs which includes only FPs generated based on the same name can be viewed <a href="FPs_same_name.htm">here</a>. It can be seen that such FPs were found by multiple tools in most cases and in some cases even by all the tools.
</p>
<p>
	We provided our general suggestions why a mapping could have been found by a tool. One of the area of future improvement for the tools can be to provide explanations for the mappings they generate. Such explanation is already included be ALOD2Vec, DESKMatcher a Wiktionary. We compared their explanation with our suggestions and in most cases, the explanations correspond. An Excel sheet with this comparison can be downloaded <a href="oaei.ontologymatching.org/2020/conference/eval_data/FPs_with_explanations.xlsx">here</a>.
</p>
<p>
	Compared to the last year's FP evaluation, we were not evaluating the reason why the FPs were incorrect mappings. We also narrowed down the "why was an alignment discovered" to 4 categories, eliminating the "structure" category. On the other hand, we added the comparison of "why was an alignment discovered" assigned by us with the explanation for the alignment provided by the system itself (for the 3 systems which generate explanations with the mappings). Looking at the systems in the last year's FP evaluation which were part of this year's FP evaluation, the results are similar. Most FPs generated by ALIN still have "same name" as the reason why they were discovered. AML, Lily, LogMAp, LogMapLt and Wiktionary still have different kinds. Just like last year, ALIN, AML, LogMap and LogMapLt have only few FPs which were not found by other systems, while Lily has many of its FPs which were not found by other systems. On the other hand, Wiktionary generated only 1 unique mapping this years while last year, it included many mappings not found by other tools. Looking at the systems which didn't participate last year (ALOD2Vec, ATBox, DESKMatcher and VeeAlign), most of their FPs were discovered because of containing the same word - this may or may not include DESKMatcher for which we didn't evaluate all of its mappings. ALOD2Vec and VeeAlign didn't generate any FPs which were discovered because of "synonyms". All four of these systems also generated a number of unique FPs.
</p>

-->
<h3><a name="uncertain-ra">Evaluation Based on the Uncertain Version of the Reference Alignment</a></h3>

<h4><a name="uncsetting">Evaluation Setting</a></h4>

<p>The confidence values of all matches in the standard (sharp) reference alignments for the conference track are all 1.0. For the uncertain version of this track, the confidence value of a match has been set equal to the percentage of a group of people who agreed with the match in question (this uncertain version is based on reference alignment labeled ra1). One key thing to note is that the group was only asked to validate matches that were already present in the existing reference alignments - so some matches had their confidence value reduced from 1.0 to a number near 0, but no new matches were added.</p>

<p>There are two ways that we can evaluate alignment systems according to these `uncertain' reference alignments, which we refer to as discrete and continuous. The discrete evaluation considers any match in the reference alignment with a confidence value of 0.5 or greater to be fully correct and those with a confidence less than 0.5 to be fully incorrect. Similarly, an alignment system’s match is considered a `yes' if the confidence value is greater than or equal to the system’s threshold and a `no' otherwise. In essence, this is the same as the `sharp' evaluation approach, except that some matches have been removed because less than half of the crowdsourcing group agreed with them. The continuous evaluation strategy penalizes an alignment system more if it misses a match on which most people agree than if it misses a more controversial match. For instance, if A = B with a confidence of 0.85 in the reference alignment and an alignment algorithm gives that match a confidence of 0.40, then that is counted as 0.85 * 0.40 = 0.34 of a true positive and 0.85 – 0.40 = 0.45 of a false negative.
</p>

<h4><a name="uncresults">Results</a></h4>
<p>
Below is a graph showing the F-measure, precision, and recall of the different alignment systems when evaluated using the sharp (s), discrete uncertain (d), and continuous uncertain (c) metrics, along with a table containing the same information. The results from this year show that more systems are assigning nuanced confidence values to the matches they produce.
</p>

<img src="./conference_files/uncertainGraph.png" alt="graph for uncertain reference alignment based evalation">
<img src="./conference_files/uncra1-M3.png" alt="results for uncertain reference alignment based evalation">


<p>
This year, out of the 7 alignment systems, 5 (ALIN, LogMapLt, MDMapper, OntoMatch, TOMATO) use 1.0 as the confidence value for all matches they identify. The remaining 2 systems (LogMap, Matcha) have a wide variation of confidence values.
</p>

<h4><a name="uncdiscussion">Discussion for Evaluation Based on the Uncertain Reference Alignments</a></h4>
<p>
The evaluation results based on the uncertain reference alignments reveal interesting insights into the behavior of different matchers when transitioning from the sharp to the uncertain metrics, both in discrete and continuous settings.
</p>
<p>
Starting with ALIN, we observe a significant improvement in performance when moving from the sharp to the uncertain evaluations. Its precision remains consistently high (0.88), but its F-measure and recall improve markedly in the discrete and continuous evaluations. This suggests that while ALIN identified fewer matches in the sharp setting, it excelled when considering uncertain matches, especially when crowd consensus was factored in. ALIN's ability to adapt to the continuous evaluation shows its robustness in handling uncertain matches, as its F-measure improves from 0.61 to 0.71, and recall increases from 0.47 to 0.60.
</p>
<p>
LogMap demonstrates a balanced performance across all metrics, with small but meaningful changes. Its precision remains stable at 0.81 in both the sharp and discrete evaluations, but it slightly drops in the continuous metric (0.80). However, LogMap's recall improves in both the discrete and continuous settings (0.58 to 0.62 and 0.57, respectively). This highlights LogMap's capacity to handle uncertain alignments better than most matchers, though it still assigns lower confidence to some high-consensus matches, which limits its precision slightly.

</p>
<p>
LogMapLt shows significant improvement in recall when moving from the sharp to the discrete and continuous metrics. In the sharp setting, its recall was only 0.50, but it climbs to 0.62 in the discrete and 0.63 in the continuous metrics. This indicates that LogMapLt was conservative in the sharp evaluation but gained confidence when handling uncertain alignments. However, its precision is slightly lower compared to others (0.73 in sharp and discrete, 0.72 in continuous), reflecting its tendency to assign lower confidence values to some correct matches.
</p>

<p>
Matcha presents an interesting case where its performance differs substantially between sharp, discrete, and continuous settings. While it performs well in terms of recall in the sharp evaluation (0.67), its precision drops in the discrete uncertain setting (0.65). However, its recall improves to 0.77, indicating that it successfully identifies uncertain matches. In the continuous evaluation, its F-measure and recall remain relatively high (0.71 and 0.75), showing that Matcha adapts well to the uncertain framework, albeit with a more cautious approach compared to its peers.
</p>

<p>
MAMapper exhibits stable recall across all metrics, from 0.55 in the sharp to 0.64 in both the discrete and continuous evaluations. Its precision, however, drops slightly from 0.71 in sharp to 0.66 in the discrete setting and 0.69 in the continuous setting. This suggests that while MAMapper is effective at recalling uncertain matches, it struggles to assign high confidence to them, which negatively impacts its precision.
</p>

<p>
OntoMatch behaves similarly to ALIN in terms of precision (0.88 across all metrics), but its F-measure and recall also improve in the discrete and continuous evaluations, from 0.46 in sharp to 0.57 and 0.59, respectively. Like ALIN, OntoMatch is conservative in the sharp evaluation but excels when incorporating uncertainty, reflecting its capability to adapt to uncertain data.
</p>

<p>
Finally, TOMATO demonstrates relatively consistent performance across all metrics but remains the weakest performer overall. Its recall increases slightly from 0.44 in sharp to 0.56 in both the discrete and continuous settings, but its precision and F-measure stay relatively low. TOMATO's results suggest that while it adapts to uncertainty, it struggles to capture high-consensus matches with confidence, which limits its overall performance.
</p>

<p>
In summary, the continuous evaluation provides a more nuanced view of each matcher’s strengths and weaknesses in handling uncertain reference alignments. Matchers like ALIN, LogMap, and OntoMatch show the most improvement when moving from sharp to uncertain evaluations, indicating their adaptability to more realistic, uncertain data. Conversely, matchers like TOMATO and MAMapper struggle with precision when handling uncertain matches, even if they exhibit stable recall. These results highlight the importance of considering confidence values in both the system's output and the reference alignments for a more comprehensive evaluation.
</p>



<h3><a name="logical">Evaluation Based on Logical Reasoning</a></h3>
<p>
For evaluation based on logical reasoning we applied detection of conservativity and consistency principles violations [2, 3]. While consistency principle proposes that correspondences should not lead to unsatisfiable classes in the merged ontology, conservativity principle proposes that correspondences should not introduce new semantic relationships between concepts from one of input ontologies [2].
</p>
<p>
Table below summarizes statistics per matcher. There are number of alignments (#Incoh.Align.) that cause unsatisfiable TBox after ontologies merge, total number of all conservativity principle violations within all alignments (#TotConser.Viol.) and its average per one alignment (#AvgConser.Viol.), total number of all consistency principle violations (#TotConsist.Viol.) and its average per one alignment (#AvgConsist.Viol.).
</p>
<p>
Comparing to the last year only three tools (ALIN, LogMap, and OntoMatch) have no consistency principle violation while four tools have some consistency principle violations. 
Conservativity principle violations are made by all tools. Two tools (ALIN, OntoMatch) have very low number (2). Further four tools (LogMap, LogMapLt, Matcha, and MDMapper) have low numbers (less than 100). One tool (TOMATO) have more than 100 conservativity principle violations.
However, we should note that these conservativity principle violations can be "false positives" since the entailment in the aligned ontology can be correct although it was not derivable in the single input ontologies.
</p>

<div id="tab_wrapper" class="dataTables_wrapper no-footer"><div id="tab_filter" class="dataTables_filter"><label>Search:<input type="search" class="" placeholder="" aria-controls="tab"></label></div>
<table id="tab" class="display" cellspacing="0" width="100%">
  <thead>
   <tr>
    <th>Matcher</th>
    <th>#Align.</th>
    <th>#Incoh.Align.</th>
    <th>#TotConser.Viol.</th>
    <th>#AvgConser.Viol.</th>
    <th>#TotConsist.Viol.</th>
    <th>#AvgConsist.Viol.</th>
   </tr>
  </thead>
  <tbody>
   <tr>
    <td>ALIN</td>
    <td>21</td>
    <td>0</td>
    <td>2</td>
    <td>0.1</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>LogMap</td>
    <td>21</td>
    <td>0</td>
    <td>21</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>LogMapLt</td>
    <td>21</td>
    <td>3</td>
    <td>97</td>
    <td>4.62</td>
    <td>18</td>
    <td>0.86</td>
   </tr>
   <tr>
    <td>MATCHA</td>
    <td>21</td>
    <td>7</td>
    <td>86</td>
    <td>4.3</td>
    <td>72</td>
    <td>3.6</td>
   </tr>
   <tr>
    <td>MDMapper</td>
    <td>21</td>
    <td>2</td>
    <td>29</td>
    <td>1.38</td>
    <td>13</td>
    <td>0.62</td>
   </tr>
   <tr>
    <td>OntoMatch</td>
    <td>21</td>
    <td>0</td>
    <td>2</td>
    <td>0.1</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>TOMATO</td>
    <td>21</td>
    <td>12</td>
    <td>353</td>
    <td>16.81</td>
    <td>162</td>
    <td>7.71</td>
   </tr>   
  </tbody>
</table>
</div>

<h4>Statistics of Consistency and Conservativity Principle Violations</h4>

<p>Here we list ten most frequent unsatisfiable classes appeared after ontologies merge by any tool. Four tools generated incoherent alignments.</p>

<pre>
iasted#Worker_non_speaker - 4
iasted#Student_registration_fee - 4
iasted#Student_non_speaker - 4
iasted#Nonauthor_registration_fee - 4
iasted#Non_speaker - 4
ekaw#Workshop_Session - 4
ekaw#Session - 4
ekaw#Rejected_Paper - 4
ekaw#Regular_Session - 4
ekaw#Poster_Session - 4
</pre>

<p>Here we list ten most frequent unsatisfiable classes appeared after ontologies merge by ontology pairs. These unsatisfiable classes were appeared in all ontology pairs for given ontology:</p>

<pre>
edas#Reviewer - 5
edas#Review - 5
cmt#Reviewer - 5
cmt#Review - 5
cmt#PaperFullVersion - 5
cmt#PaperAbstract - 5
cmt#Paper - 5
cmt#Meta-Reviewer - 5
cmt#Meta-Review - 5
cmt#ExternalReviewer - 5
</pre>

<p>Here we list top 10 conservativity principle violations by any tool:</p>
<!--Here we list ten most frequent caused new semantic relationships between concepts within input ontologies by any tool:-->

<pre>
iasted#Record_of_attendance, iasted#City - 7
	edas-iasted
conference#Invited_speaker, conference#Conference_participant - 6
	conference-ekaw
iasted#Sponzorship, iasted#Registration_fee - 5
	iasted-sigkdd
iasted#Sponzorship, iasted#Fee - 5
	iasted-sigkdd
iasted#Session_chair, iasted#Speaker - 5
	iasted-sigkdd
	ekaw-iasted
iasted#Hotel_fee, iasted#Registration_fee - 5
	iasted-sigkdd
iasted#Fee_for_extra_trip, iasted#Registration_fee - 5
	iasted-sigkdd
sigkdd#Listener, sigkdd#Speaker - 4
	iasted-sigkdd
iasted#Video_presentation, iasted#Item - 4
	conference-iasted
	edas-iasted
iasted#Technical_commitee, iasted#Speaker - 4
	iasted-sigkdd
</pre>


<!--<h3><a name="dbpedia">Experiment and Evaluation of DBpedia to OntoFarm Ontologies Matching</a></h3>
<p>For this subtrack we have three experimental test cases with regard to matching the cross-domain DBpedia ontology to three OntoFarm ontologies. We merely focus on entities of DBpedia ontology (dbo) from DBpedia namespace (therefore we prepared filtered DBpedia ontology - this is different than the last year), i.e. dbpedia.org/ontology/ and three selected ontologies from OntoFarm: confof, ekaw, sigkdd. as explained in [5].</p>

<p>Out of 12 systems 6 managed to match dbpedia to OntoFarm ontologies (ATMatcher, KGMatcher+, LogMap, LogMapLt, LSMatch, and Matcha).</p>
<p>We evaluated alignments from the systems and the results are in the table below. Additionally, we added two baselines: StringEquiv is a string matcher based on string equality applied on local names of entities which were lowercased and edna (string editing distance matcher).</p>
<img src="eval_data/dbpedia-results.png" alt="dbpedia evaluation results">
<p>We can see four systems perform better than two baselines. LogMap dominates with 0.61 of F1-measure. Most systems achieve lower scores of measures than in the case of matching domain ontologies except KGMatcher+. This shows that these test cases are more difficult for traditional ontology matching systems.
</p>-->

<h2><a name="organizers">Organizers</a></h2>
<ul>
<li>Ondřej Zamazal (Prague University of Economics and Business, CZ), main contact for the track, ondrej dot zamazal at vse dot cz
</li><li>Jana Vataščinová (Prague University of Economics and Business, CZ)
</li><li>Lu Zhou (Flatfee Corp, USA)
</li></ul>


<h2><a name="references">References</a></h2>

<p>[1] Michelle Cheatham, Pascal Hitzler: Conference v2.0: An Uncertain 
Version of the OAEI Conference Benchmark. International Semantic Web 
Conference (2) 2014: 33-48.</p>
<p>[2] Alessandro Solimando, Ernesto Jiménez-Ruiz, Giovanna Guerrini:
Detecting and Correcting Conservativity Principle Violations in 
Ontology-to-Ontology Mappings. International Semantic Web Conference (2)
 2014: 1-16.</p>
<p>[3] Alessandro Solimando, Ernesto Jiménez-Ruiz, Giovanna Guerrini: A 
Multi-strategy Approach for Detecting and Correcting Conservativity 
Principle Violations in Ontology Alignments. OWL: Experiences and 
Directions Workshop 2014 (OWLED 2014). 13-24.
</p>
<p>[4] Ondřej Zamazal, Vojtěch Svátek. The Ten-Year OntoFarm and its 
Fertilization within the Onto-Sphere. Web Semantics: Science, Services 
and Agents on the World Wide Web, 43, 46-53. 2018.
<!--<p>[5] Martin Šatra, Ondřej Zamazal. Towards Matching of Domain Ontologies to
Cross-Domain Ontology: Evaluation Perspective. Ontology Matching 2020.</p>
</p>-->

</p><div class="address">
<div class="footer"><a href="oaei.ontologymatching.org/2024/conference/index.html">oaei.ontologymatching.org/2024/conference/index.html</a></div>
<!--$Id$-->
</div>

</body></html>
